{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/data/tim/heronWorkspace/src\")\n",
    "sys.path.append(\"/data/tim/heronWorkspace/AE\")\n",
    "sys.path.append(\"/data/tim/heronWorkspace/classifier\")\n",
    "sys.path.append(\"/data/tim/heronWorkspace/\")\n",
    "\n",
    "\n",
    "from AEHeronModelV1 import AEHeronModel\n",
    "from AEHeronModelV2 import CAEHeron\n",
    "from lightning.pytorch.callbacks import ModelSummary\n",
    "from torchsummary import summary\n",
    "import HeronImageLoader\n",
    "from torch.utils.data import DataLoader, BatchSampler\n",
    "from matplotlib import pyplot as plt\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "import pandas as pd\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from MLPV1 import MLP, MLPMSEHeatMap\n",
    "from models import MLPBasic, CAEBigBottleneck, CAESmallBottleneckWithLinear, MLPBasicHeatMap, CAEV1\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision.transforms import GaussianBlur\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n",
    "from scipy.stats import loguniform\n",
    "from ClassifierDatasets import DatasetThreeConsecutive, UnNormalize\n",
    "# from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import loguniform\n",
    "import functorch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "# from torchmetrics import ConfusionMatrix\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params:\n",
    "- different CAE\n",
    "- cameraProps - cutoff on the sides\n",
    "- gaussian filter params / min-filter params\n",
    "- zero thresholds\n",
    "- sum threshold\n",
    "- dataset props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"batch_size\":         16\n",
      "\"bottleneck\":         128\n",
      "\"cameras\":            ['NEN1', 'SBU3']\n",
      "\"gammaScheduler\":     0.7879353477950641\n",
      "\"ldim\":               16\n",
      "\"learning_rate\":      0.003533238255084643\n",
      "\"num_workers_loader\": 4\n",
      "\"transforms\":         None\n",
      "\"weight_decay\":       4.5387898639621974e-07\n"
     ]
    }
   ],
   "source": [
    "loaderParams = dict(\n",
    "    lblValidationMode = \"Manual\",\n",
    "    balanced = True,\n",
    "    anomalyObviousness = \"obvious\",\n",
    "    distinctCAETraining = False,\n",
    "    colorMode = \"RGB\",\n",
    "    random_state = 1,\n",
    "    set = \"all\"\n",
    ")\n",
    "\n",
    "distributions = dict(\n",
    "    cameras = [[\"NEN1\", \"SBU3\"]], #, [\"SBU4\"]\n",
    "    balanced = [True, False],\n",
    "    distinctCAETraining = [True, False],\n",
    "    gaussianFilterSize = [5],\n",
    "    gaussianFilterSigma = [3, 5],\n",
    "    filter = [\"MinFilter\"], #[\"MinFilter\", \"GaussianFilter\"]\n",
    "    zeroThreshold = np.random.uniform(low=0.15, high=0.4, size=30), # uniform dist on loc, loc+scale -> uniform(loc, scale) #threshold for zeroing out the image\n",
    "    sumThreshold = np.random.uniform(low=20, high=50, size=30),\n",
    "    lossFn = [\"L1\"]#[\"MSE\", \"L1\"],\n",
    ")\n",
    "\n",
    "sampler = ParameterSampler(distributions, n_iter=3, random_state=loaderParams[\"random_state\"])\n",
    "\n",
    "includeCameraProps = False\n",
    "model = CAEV1\n",
    "\n",
    "checkPoint = '/data/tim/heronWorkspace/logs/CAEV1/version_3/checkpoints/epoch=49-step=19350.ckpt'\n",
    "caeLoaded = CAEHeron.load_from_checkpoint(checkPoint, model = model)\n",
    "caeLoaded.freeze()\n",
    "print(caeLoaded.hparams)\n",
    "\n",
    "fileName = \"basicCAETest\"\n",
    "columns = [\"trueLbl\", \"predictedLbl\", \"ImagePath\", \"includeCameraProps\", \"CAECheckPoint\", \"isCAETrainingCamera\"] + list(loaderParams.keys()) + list(distributions.keys()) \n",
    "try:\n",
    "    df = pd.read_csv(f\"/data/tim/heronWorkspace/caeTestData/{fileName}.csv\")\n",
    "except:\n",
    "    df = pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_filter(tensor : torch.Tensor, kernel_size=3):\n",
    "    # Unfold the tensor into sliding local blocks\n",
    "    unfolded = tensor.unfold(0, kernel_size, 1)\n",
    "    unfolded = unfolded.unfold(1, kernel_size, 1)\n",
    "    # Compute the minimum in each of these blocks\n",
    "    return unfolded.min(dim=-1)[0].min(dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[0] * 5, [1] * 5, [2] * 5, [3] * 5, [4] * 5]]).unfold(1, 3, 1)\n",
    "a.unfold(2, 3, 1)\n",
    "\n",
    "a = torch.tensor([[0] * 5, [1] * 5, [2] * 5, [3] * 5, [4] * 5]).unfold(0, 3, 1)\n",
    "a.unfold(1, 3, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try: unnorm only on result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 62\n",
      "{'zeroThreshold': 0.2455403529275042, 'sumThreshold': 27.85840753027775, 'lossFn': 'L1', 'gaussianFilterSize': 5, 'gaussianFilterSigma': 5, 'filter': 'MinFilter', 'distinctCAETraining': True, 'cameras': ['NEN1', 'SBU3'], 'balanced': False}\n"
     ]
    }
   ],
   "source": [
    "#TODO: include camerProps\n",
    "\n",
    "# confMat = ConfusionMatrix(task=\"binary\", num_classes=2)\n",
    "unnorm = UnNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "\n",
    "negVals = {\n",
    "        \"mean\": [],\n",
    "        \"std\": [],\n",
    "        \"min\": [],\n",
    "        \"max\": []\n",
    "    }\n",
    "posVals = {\n",
    "    \"mean\": [],\n",
    "    \"std\": [],\n",
    "    \"min\": [],\n",
    "    \"max\": []\n",
    "}\n",
    "\n",
    "for params in sampler:\n",
    "    dataset = DatasetThreeConsecutive(cameras=params[\"cameras\"], resize_to=CAEV1.imsize, **loaderParams)\n",
    "    print(f'Length of dataset: {len(dataset)}')\n",
    "    print(params)\n",
    "    dataLoader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    blur = GaussianBlur(kernel_size=5, sigma=3) #TODO: make this a parameter\n",
    "    lossFn = F.mse_loss if params[\"lossFn\"] == \"MSE\" else F.l1_loss\n",
    "    vals = []\n",
    "\n",
    "    for (imArr, lblArr, camera, ImagePath) in dataLoader:\n",
    "        isTrainingCamera = camera in caeLoaded.hparams.cameras\n",
    "        prevImg = imArr[0] #alwasy #batch_size images\n",
    "        currImg = imArr[1]\n",
    "        nextImg = imArr[2]\n",
    "\n",
    "        prevPred, currPred, nextPred = [unnorm(caeLoaded(x.to(caeLoaded.device))) for x in [prevImg, currImg, nextImg]]\n",
    "        prevImg, currImg, nextImg = [unnorm(x) for x in [prevImg, currImg, nextImg]]\n",
    "\n",
    "        # plt.imshow(currImg[0].cpu().detach().numpy().transpose(1, 2, 0))\n",
    "        # plt.show()\n",
    "\n",
    "        prevImgBlurred, currImgBlurred, nextImgBlurred = [blur.forward(x).to(prevPred.device) for x in [prevImg, currImg, nextImg]]\n",
    "        # plt.imshow(currImgBlurred[0].cpu().detach().numpy().transpose(1, 2, 0))\n",
    "        # plt.show()\n",
    "        \n",
    "        prevImd, currImd, nextImd = [torch.sum(lossFn(imgBlurred, pred, reduction='none'), dim=1) for imgBlurred, pred in zip([prevImgBlurred, currImgBlurred, nextImgBlurred], [prevPred, currPred, nextPred])]\n",
    "\n",
    "        prevToCurrImd = torch.clamp(torch.sub(currImd, prevImd), min= 0)\n",
    "        nextToCurrImd = torch.clamp(torch.sub(currImd, nextImd), min= 0)\n",
    "\n",
    "        prevNextCurrImd = torch.div(torch.add(prevToCurrImd, nextToCurrImd), 2)\n",
    "\n",
    "        prevNextCurrImdMin = torch.stack([min_filter(x, kernel_size=3) for x in prevNextCurrImd]) #TODO: evtl make this as before\n",
    "        \n",
    "\n",
    "        # plt.imshow(prevNextCurrImdMin[0].cpu().detach().numpy(), cmap=\"hot\")\n",
    "        # print(torch.sum(prevNextCurrImdMin[0]).item())\n",
    "        # plt.show()\n",
    "        # plt.imshow(prevNextCurrImdMin[1].cpu().detach().numpy(), cmap=\"hot\")\n",
    "        # print(torch.sum(prevNextCurrImdMin[1]).item())\n",
    "        # plt.show()\n",
    "\n",
    "        # prevNextCurrImdMin = torch.div(prevNextCurrImdMin, 255)\n",
    "        # plt.imshow(prevNextCurrImdMin[0].cpu().detach().numpy(), cmap=\"hot\")\n",
    "        # plt.show()\n",
    "\n",
    "        # print(torch.max(prevNextCurrImdMin))\n",
    "        prevNextCurrImdMinThresh = torch.where(prevNextCurrImdMin < params[\"zeroThreshold\"], torch.zeros_like(prevNextCurrImdMin), prevNextCurrImdMin)\n",
    "        # plt.imshow(prevNextCurrImdMinThresh[0].cpu().detach().numpy(), cmap=\"hot\")\n",
    "        # plt.show()\n",
    "\n",
    "        sumPrevNextCurrImdMin = torch.sum(prevNextCurrImdMinThresh, dim=(1, 2))\n",
    "        predictions = (sumPrevNextCurrImdMin> params[\"sumThreshold\"]).to(torch.int)\n",
    "\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            vals.append([lblArr[i].item(), predictions[i].item(), sumPrevNextCurrImdMin[i].item()])\n",
    "        #     dfNew = pd.DataFrame(columns=columns, )\n",
    "        #     dfNew = df.append(dict(zip(columns, [lblArr[i].item(), predictions[i].item(), ImagePath[i], includeCameraProps, checkPoint, isTrainingCamera] + list(loaderParams.values()) + list(params.values()))), ignore_index=True)\n",
    "        \n",
    "        # df.to_csv(f\"/data/tim/heronWorkspace/caeTestData/{fileName}.csv\", index=False)\n",
    "\n",
    "        # print(predictions)\n",
    "        # print(prevNextCurrImdMin.shape)\n",
    "        # print(torch.max(prevNextCurrImdMin))\n",
    "        # print(imArr[0].shape)\n",
    "        # print(len(imArr))\n",
    "        # print(len(lblArr))\n",
    "\n",
    "    # confMat\n",
    "    vals = np.array(vals)\n",
    "    # print(vals)\n",
    "    # print(vals[:, 0])\n",
    "    # print(vals[:, 1])\n",
    "    confMat = confusion_matrix(vals[:, 0], vals[:,1])\n",
    "    fig= plt.figure(figsize = (6,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.heatmap(confMat, annot=True)\n",
    "    ax.set_xlabel('Pred', fontsize=12)\n",
    "    ax.set_ylabel('True', fontsize=12)\n",
    "    onlyPos = vals[vals[:, 0] == 1]\n",
    "    onlyNeg = vals[vals[:, 0] == 0]\n",
    "    print(f'mean pos: {np.mean(onlyPos[:, 2])}, std pos: {np.std(onlyPos[:, 2])}')\n",
    "    print(f'mean neg: {np.mean(onlyNeg[:, 2])}, std neg: {np.std(onlyNeg[:, 2])}')\n",
    "    print(f'lowest pos: {np.min(onlyPos[:, 2])}, highest pos: {np.max(onlyPos[:, 2])}')\n",
    "    print(f'lowest neg: {np.min(onlyNeg[:, 2])}, highest neg: {np.max(onlyNeg[:, 2])}')\n",
    "\n",
    "    posVals[\"mean\"].append(np.mean(onlyPos[:, 2]))\n",
    "    posVals[\"std\"].append(np.std(onlyPos[:, 2]))\n",
    "    posVals[\"min\"].append(np.min(onlyPos[:, 2]))\n",
    "    posVals[\"max\"].append(np.max(onlyPos[:, 2]))\n",
    "\n",
    "    negVals[\"mean\"].append(np.mean(onlyNeg[:, 2]))\n",
    "    negVals[\"std\"].append(np.std(onlyNeg[:, 2]))\n",
    "    negVals[\"min\"].append(np.min(onlyNeg[:, 2]))\n",
    "    negVals[\"max\"].append(np.max(onlyNeg[:, 2]))\n",
    "\n",
    "    print(f'accuracy: {accuracy_score(vals[:, 0], vals[:,1])}')\n",
    "    print(f'precision: {precision_score(vals[:, 0], vals[:,1])}')\n",
    "    print(f'recall: {recall_score(vals[:, 0], vals[:,1])}')\n",
    "    print(f'f1: {f1_score(vals[:, 0], vals[:,1])}')\n",
    "    plt.show()\n",
    "    vals = []\n",
    "\n",
    "        \n",
    "fig = plt.figure(figsize = (6,3))\n",
    "plt.errorbar(range(len(posVals[\"mean\"])), posVals[\"mean\"], yerr=posVals[\"std\"], fmt='o', label=\"pos\", color=\"blue\")\n",
    "plt.errorbar(range(len(negVals[\"mean\"])), negVals[\"mean\"], yerr=negVals[\"std\"], fmt='-', label=\"neg\", color=\"red\")\n",
    "plt.plot(range(len(posVals[\"mean\"])), posVals[\"max\"], 'o', color=\"blue\")\n",
    "plt.plot(range(len(negVals[\"mean\"])), negVals[\"max\"], '-', color=\"red\")\n",
    "plt.plot(range(len(posVals[\"mean\"])), posVals[\"min\"], 'o', color=\"blue\")\n",
    "plt.plot(range(len(negVals[\"mean\"])), negVals[\"min\"], '-', color=\"red\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.xlim([-1, 5])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# confusionMatrix\n",
    "# confMat = confusion_matrix(df[\"trueLbl\"], df[\"predictedLbl\"])\n",
    "# plt.figure(figsize = (6,3))\n",
    "# sns.heatmap(confMat, annot=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(posVals[\"mean\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"./resultsBasicClassifier.csv\")\n",
    "except:\n",
    "    df = pd.DataFrame(columns = params.keys(), )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heronsTim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
